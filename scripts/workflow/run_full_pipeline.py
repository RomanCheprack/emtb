#!/usr/bin/env python3
"""
Full pipeline orchestrator for eMTB data processing workflow.

This script runs the complete data pipeline:
1. Run all scrapers
2. Check duplicates in raw JSON files
3. Standardize data
4. Deduplicate standardized data
5. Drop bike data from database
6. Migrate new data to database
7. Send email notification

All steps run in a transaction for safe rollback on failure.
"""

import sys
import os
import subprocess
import time
from datetime import datetime
from pathlib import Path

# Add project root to path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, project_root)

from dotenv import load_dotenv
from app import create_app
from app.extensions import db
from app.models import Bike, Brand, Source, BikeListing
# Import email notifier
sys.path.insert(0, os.path.join(project_root, 'scripts', 'utils'))
from email_notifier import send_pipeline_notification


class PipelineStats:
    """Track pipeline execution statistics"""
    def __init__(self):
        self.stats = {}
        self.start_time = time.time()
    
    def add_step(self, step_name, status, **kwargs):
        """Add step statistics"""
        self.stats[step_name] = {
            'status': status,
            **kwargs
        }
    
    def get_duration(self):
        """Get total execution duration in seconds"""
        return time.time() - self.start_time
    
    def to_dict(self):
        """Convert to dictionary for email notification"""
        return self.stats


def run_command(command, description, cwd=None):
    """
    Run a shell command and return (success, output, error).
    
    Args:
        command: Command to run (list of strings)
        description: Description of what's being run
        cwd: Working directory (defaults to project root)
    
    Returns:
        (success: bool, stdout: str, stderr: str)
    """
    if cwd is None:
        cwd = project_root
    
    print(f"\n{'='*80}")
    print(f"‚ñ∂Ô∏è  {description}")
    print(f"{'='*80}")
    
    try:
        # Set UTF-8 encoding for subprocess output (Windows compatibility)
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        env['PYTHONUTF8'] = '1'
        
        result = subprocess.run(
            command,
            cwd=cwd,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='replace',  # Replace encoding errors instead of failing
            timeout=3600,  # 1 hour timeout
            env=env
        )
        
        if result.returncode == 0:
            print(f"‚úÖ {description} completed successfully")
            if result.stdout:
                # Print last 20 lines of output
                lines = result.stdout.strip().split('\n')
                if len(lines) > 20:
                    print("   ... (showing last 20 lines)")
                    for line in lines[-20:]:
                        print(f"   {line}")
                else:
                    for line in lines:
                        print(f"   {line}")
            return True, result.stdout, result.stderr
        else:
            print(f"‚ùå {description} failed with return code {result.returncode}")
            if result.stderr:
                print(f"   Error: {result.stderr[:500]}")
            return False, result.stdout, result.stderr
            
    except subprocess.TimeoutExpired:
        print(f"‚è∞ {description} timed out after 1 hour")
        return False, "", "Timeout after 1 hour"
    except Exception as e:
        print(f"‚ùå Error running {description}: {e}")
        return False, "", str(e)


def step_1_run_scrapers(stats):
    """Step 1: Run all scrapers"""
    print("\n" + "="*80)
    print("STEP 1: RUNNING SCRAPERS")
    print("="*80)
    
    scraper_script = os.path.join(project_root, 'scripts', 'scrapers', 'run_all_scrapers.py')
    command = [sys.executable, scraper_script]
    
    success, stdout, stderr = run_command(command, "Running all scrapers")
    
    # Parse output to get statistics
    total_entries = 0
    scrapers_run = 0
    
    if success:
        # Try to extract entry counts from output
        lines = stdout.split('\n')
        for line in lines:
            if 'Entries scraped:' in line:
                try:
                    count = int(line.split('Entries scraped:')[1].strip().split()[0])
                    total_entries += count
                    scrapers_run += 1
                except:
                    pass
    
    stats.add_step('scraping', 'success' if success else 'failed',
                   scrapers_run=scrapers_run,
                   total_entries=total_entries,
                   error=stderr if not success else None)
    
    return success


def step_2_check_duplicates(stats):
    """Step 2: Check duplicates in raw JSON files"""
    print("\n" + "="*80)
    print("STEP 2: CHECKING DUPLICATES IN RAW JSON")
    print("="*80)
    
    duplicate_script = os.path.join(project_root, 'scripts', 'data', 'check_duplicate_entries.py')
    command = [sys.executable, duplicate_script]
    
    success, stdout, stderr = run_command(command, "Checking duplicates in raw JSON files")
    
    # Parse output to get statistics
    files_processed = 0
    total_entries = 0
    duplicates_removed = 0
    
    if success:
        lines = stdout.split('\n')
        for line in lines:
            if 'Total entries processed:' in line:
                try:
                    total_entries = int(line.split('Total entries processed:')[1].strip())
                except:
                    pass
            elif 'Duplicates removed:' in line:
                try:
                    duplicates_removed = int(line.split('Duplicates removed:')[1].strip())
                except:
                    pass
            elif 'Found' in line and 'file(s) to process' in line:
                try:
                    files_processed = int(line.split('Found')[1].split('file(s)')[0].strip())
                except:
                    pass
    
    stats.add_step('duplicate_check', 'success' if success else 'failed',
                   files_processed=files_processed,
                   total_entries=total_entries,
                   duplicates_removed=duplicates_removed,
                   error=stderr if not success else None)
    
    return success


def step_3_standardize(stats):
    """Step 3: Standardize JSON files"""
    print("\n" + "="*80)
    print("STEP 3: STANDARDIZING DATA")
    print("="*80)
    
    standardize_script = os.path.join(project_root, 'scripts', 'data', 'standardize_json.py')
    command = [sys.executable, standardize_script]
    
    success, stdout, stderr = run_command(command, "Standardizing JSON files")
    
    # Parse output to get statistics
    files_processed = 0
    
    if success:
        lines = stdout.split('\n')
        for line in lines:
            if 'Found' in line and 'JSON files to standardize' in line:
                try:
                    files_processed = int(line.split('Found')[1].split('JSON files')[0].strip())
                except:
                    pass
    
    stats.add_step('standardization', 'success' if success else 'failed',
                   files_processed=files_processed,
                   error=stderr if not success else None)
    
    return success


def step_5_drop_data(stats, app):
    """Step 5: Drop bike data from database"""
    print("\n" + "="*80)
    print("STEP 5: DROPPING BIKE DATA")
    print("="*80)
    
    drop_script = os.path.join(project_root, 'scripts', 'db', 'drop_bike_data.py')
    command = [sys.executable, drop_script, '--force']
    
    success, stdout, stderr = run_command(command, "Dropping bike data from database")
    
    # Parse output to get statistics
    bikes_deleted = 0
    listings_deleted = 0
    
    if success:
        lines = stdout.split('\n')
        for line in lines:
            if 'Deleted' in line and 'bikes' in line:
                try:
                    bikes_deleted = int(line.split('Deleted')[1].split('bikes')[0].strip())
                except:
                    pass
            elif 'Deleted' in line and 'listings' in line:
                try:
                    listings_deleted = int(line.split('Deleted')[1].split('listings')[0].strip())
                except:
                    pass
    
    stats.add_step('drop_data', 'success' if success else 'failed',
                   bikes_deleted=bikes_deleted,
                   listings_deleted=listings_deleted,
                   error=stderr if not success else None)
    
    return success


def step_6_migrate(stats, app):
    """Step 6: Migrate data to database"""
    print("\n" + "="*80)
    print("STEP 6: MIGRATING DATA TO DATABASE")
    print("="*80)
    
    migrate_script = os.path.join(project_root, 'scripts', 'db', 'migrate_to_mysql.py')
    command = [sys.executable, migrate_script, '--force']
    
    success, stdout, stderr = run_command(command, "Migrating data to database")
    
    # Parse output and get database statistics
    bikes_added = 0
    bikes_skipped = 0
    brands_count = 0
    sources_count = 0
    
    if success:
        # Get actual counts from database
        with app.app_context():
            bikes_added = Bike.query.count()
            brands_count = Brand.query.count()
            sources_count = Source.query.count()
        
        # Try to parse skipped count from output
        lines = stdout.split('\n')
        for line in lines:
            if 'Bikes skipped' in line or 'skipped (duplicates)' in line:
                try:
                    bikes_skipped = int(line.split('skipped')[0].split()[-1])
                except:
                    pass
    
    stats.add_step('migration', 'success' if success else 'failed',
                   bikes_added=bikes_added,
                   bikes_skipped=bikes_skipped,
                   brands_count=brands_count,
                   sources_count=sources_count,
                   error=stderr if not success else None)
    
    return success


def main():
    """Main pipeline execution"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Run full data pipeline')
    parser.add_argument('--skip-scrapers', action='store_true', 
                       help='Skip scraping step (use existing raw data)')
    parser.add_argument('--dry-run', action='store_true',
                       help='Dry run mode (does not modify data)')
    
    args = parser.parse_args()
    
    # Load environment variables
    load_dotenv(override=True)
    
    # Create Flask app
    app = create_app()
    
    print("="*80)
    print("üöÄ EMTB DATA PIPELINE")
    print("="*80)
    print(f"\nStarted at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Project root: {project_root}")
    
    if args.dry_run:
        print("\nüîç DRY RUN MODE - No data will be modified")
    
    if args.skip_scrapers:
        print("\n‚è≠Ô∏è  Skipping scraping step (using existing data)")
    
    # Initialize statistics
    stats = PipelineStats()
    
    # Track overall success
    pipeline_success = True
    error_message = None
    
    try:
        # Step 1: Run scrapers
        if not args.skip_scrapers:
            if not step_1_run_scrapers(stats):
                pipeline_success = False
                error_message = "Scraping failed"
                raise Exception("Pipeline stopped at scraping step")
        else:
            stats.add_step('scraping', 'skipped')
        
        # Step 2: Check duplicates
        if not step_2_check_duplicates(stats):
            pipeline_success = False
            error_message = "Duplicate check failed"
            raise Exception("Pipeline stopped at duplicate check step")
        
        # Step 3: Standardize
        if not step_3_standardize(stats):
            pipeline_success = False
            error_message = "Standardization failed"
            raise Exception("Pipeline stopped at standardization step")
        
        # Step 4: Deduplicate (SKIPPED - duplicates already removed in raw stage)
        # No need to deduplicate standardized data since duplicates were removed in step 2
        stats.add_step('deduplication', 'skipped',
                      note='Duplicates already removed in raw JSON stage')
        
        # Step 5: Drop data (only if not dry-run)
        if not args.dry_run:
            if not step_5_drop_data(stats, app):
                pipeline_success = False
                error_message = "Drop data failed"
                raise Exception("Pipeline stopped at drop data step")
        else:
            stats.add_step('drop_data', 'skipped')
            print("\n‚è≠Ô∏è  Skipping drop data step (dry-run mode)")
        
        # Step 6: Migrate (only if not dry-run)
        if not args.dry_run:
            if not step_6_migrate(stats, app):
                pipeline_success = False
                error_message = "Migration failed"
                raise Exception("Pipeline stopped at migration step")
        else:
            stats.add_step('migration', 'skipped')
            print("\n‚è≠Ô∏è  Skipping migration step (dry-run mode)")
        
        # All steps completed
        duration = stats.get_duration()
        duration_min = int(duration / 60)
        duration_sec = int(duration % 60)
        
        print("\n" + "="*80)
        print("‚úÖ PIPELINE COMPLETED SUCCESSFULLY")
        print("="*80)
        print(f"Total duration: {duration_min}m {duration_sec}s")
        
        # Send success notification
        send_pipeline_notification(
            stats.to_dict(),
            duration,
            'success'
        )
        
    except Exception as e:
        pipeline_success = False
        error_message = str(e)
        duration = stats.get_duration()
        
        print("\n" + "="*80)
        print("‚ùå PIPELINE FAILED")
        print("="*80)
        print(f"Error: {error_message}")
        print(f"Duration: {int(duration/60)}m {int(duration%60)}s")
        
        # Send failure notification
        send_pipeline_notification(
            stats.to_dict(),
            duration,
            'failed',
            error_message
        )
        
        sys.exit(1)
    
    print("\n" + "="*80)
    print("üéâ Pipeline execution complete!")
    print("="*80)


if __name__ == "__main__":
    main()

